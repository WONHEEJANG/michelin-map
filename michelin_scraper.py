import requests
from bs4 import BeautifulSoup
import time
import json
import csv
from urllib.parse import urljoin, urlparse
import re
import os
from pathlib import Path

class MichelinScraper:
    def __init__(self):
        self.base_url = "https://guide.michelin.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.restaurants = []
        self.images_dir = Path("restaurant_images")
        self.images_dir.mkdir(exist_ok=True)
        
    def get_restaurant_urls(self, start_url):
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ëª¨ë“  ìŒì‹ì  URL ìˆ˜ì§‘"""
        print("ìŒì‹ì  URL ìˆ˜ì§‘ ì¤‘...")
        restaurant_urls = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©
        page = 1
        consecutive_empty_pages = 0
        
        while consecutive_empty_pages < 2:  # ì—°ì†ìœ¼ë¡œ 2í˜ì´ì§€ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¤‘ë‹¨
            try:
                # í˜ì´ì§€ë³„ë¡œ URL ìƒì„± (ë¯¸ìŠë­ ê°€ì´ë“œ URL íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì •)
                if page == 1:
                    url = start_url
                else:
                    # ì‹¤ì œ ë¯¸ìŠë­ ê°€ì´ë“œì˜ í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ í™•ì¸
                    url = f"https://guide.michelin.com/kr/ko/seoul-capital-area/kr-seoul/restaurants/page/{page}?sort=distance"
                    
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘: {url}")
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ìŒì‹ì  ë§í¬ ì°¾ê¸° - ë” êµ¬ì²´ì ì¸ ì…€ë ‰í„° ì‚¬ìš©
                restaurant_links = soup.select('a[href*="/restaurant/"]')
                
                print(f"ì„ íƒì 'a[href*=\"/restaurant/\"]'ë¡œ {len(restaurant_links)}ê°œ ë§í¬ ë°œê²¬")
                
                if not restaurant_links:
                    consecutive_empty_pages += 1
                    print(f"í˜ì´ì§€ {page}ì—ì„œ ìŒì‹ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì—°ì† ë¹ˆ í˜ì´ì§€: {consecutive_empty_pages})")
                    page += 1
                    continue
                
                # ì´ í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ë§í¬ë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
                consecutive_empty_pages = 0
                page_urls = []
                
                for link in restaurant_links:
                    href = link.get('href')
                    if href and '/restaurant/' in href:
                        full_url = urljoin(self.base_url, href)
                        if full_url not in restaurant_urls:
                            restaurant_urls.add(full_url)
                            page_urls.append(full_url)
                
                print(f"í˜ì´ì§€ {page}ì—ì„œ {len(page_urls)}ê°œ ë ˆìŠ¤í† ë‘ ë°œê²¬")
                
                # í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸
                pagination = soup.find('nav', {'aria-label': 'pagination'}) or soup.find('div', class_=re.compile(r'pagination'))
                if pagination:
                    # í˜„ì¬ í˜ì´ì§€ê°€ ë§ˆì§€ë§‰ì¸ì§€ í™•ì¸
                    current_page_elem = pagination.find('span', class_=re.compile(r'current|active')) or \
                                       pagination.find('a', class_=re.compile(r'current|active'))
                    
                    # ë‹¤ìŒ í˜ì´ì§€ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                    all_page_links = pagination.find_all('a')
                    max_page_num = 0
                    for link in all_page_links:
                        try:
                            page_num = int(link.get_text(strip=True))
                            max_page_num = max(max_page_num, page_num)
                        except ValueError:
                            continue
                    
                    if max_page_num > 0 and page >= max_page_num:
                        print(f"í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ {max_page_num}ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                        break
                
                page += 1
                time.sleep(1)  # ìš”ì²­ ê°„ê²©
                
            except Exception as e:
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                consecutive_empty_pages += 1
                page += 1
                if consecutive_empty_pages >= 2:
                    break
        
        restaurant_urls_list = list(restaurant_urls)
        print(f"ì´ {len(restaurant_urls_list)}ê°œ ìŒì‹ì  URL ìˆ˜ì§‘ ì™„ë£Œ")
        return restaurant_urls_list
    
    def extract_image_urls(self, soup):
        """ìŒì‹ì  í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URLë“¤ ì¶”ì¶œ"""
        image_urls = []
        
        # ìŒì‹ì  ì´ë¯¸ì§€ ì„ íƒì (ìš°ì„ ìˆœìœ„ ìˆœ)
        selectors = [
            '.masthead__gallery img',  # ë©”ì¸ ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ (ê°€ì¥ ì¤‘ìš”)
            '.masthead img',  # ë§ˆìŠ¤íŠ¸í—¤ë“œ ë‚´ ì´ë¯¸ì§€
            'img[ci-src]',   # ci-src ì†ì„±ì´ ìˆëŠ” ì´ë¯¸ì§€
            'img[data-src]', # data-src ì†ì„±ì´ ìˆëŠ” ì´ë¯¸ì§€
            'img[src*="cloudimg.io"]',  # cloudimg.io ë„ë©”ì¸ì˜ ì´ë¯¸ì§€
            '.gallery img',   # ê°¤ëŸ¬ë¦¬ ë‚´ ì´ë¯¸ì§€
            '.image-gallery img',  # ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬
            '.restaurant-image img',  # ë ˆìŠ¤í† ë‘ ì´ë¯¸ì§€
            'img[alt*="restaurant"]',  # ë ˆìŠ¤í† ë‘ ê´€ë ¨ alt í…ìŠ¤íŠ¸
        ]
        
        for selector in selectors:
            image_elements = soup.select(selector)
            print(f"    ì„ íƒì '{selector}': {len(image_elements)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
            
            for img in image_elements:
                # ë‹¤ì–‘í•œ ì†ì„±ì—ì„œ URL ì¶”ì¶œ
                url_attributes = ['ci-src', 'data-src', 'src']
                
                for attr in url_attributes:
                    url = img.get(attr)
                    if url:
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if url.startswith('/'):
                            url = f"https://guide.michelin.com{url}"
                        
                        # ìŒì‹ì  ì´ë¯¸ì§€ í•„í„°ë§ (ì•„ì´ì½˜, ë¡œê³  ì œì™¸)
                        if self.is_restaurant_image(url, img):
                            # í¬ê¸° ì¡°ì • íŒŒë¼ë¯¸í„° ì œê±°
                            if '?' in url:
                                original_url = url.split('?')[0]
                            else:
                                original_url = url
                            
                            if original_url not in image_urls:
                                image_urls.append(original_url)
                                print(f"    âœ“ ìŒì‹ì  ì´ë¯¸ì§€ ë°œê²¬: {original_url[:50]}...")
        
        # ì¤‘ë³µ ì œê±°
        image_urls = list(set(image_urls))
        print(f"    ğŸ“¸ ì´ {len(image_urls)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ URL ì¶”ì¶œ")
        return image_urls
    
    def is_restaurant_image(self, url, img_element):
        """ìŒì‹ì  ì´ë¯¸ì§€ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
        # ì œì™¸í•  ì´ë¯¸ì§€ íŒ¨í„´ë“¤
        exclude_patterns = [
            'michelin-award',  # ë¯¸ì‰ë¦° ì–´ì›Œë“œ ì•„ì´ì½˜
            'icons/',  # ì•„ì´ì½˜ë“¤
            'social-',  # ì†Œì…œ ë¯¸ë””ì–´ ì•„ì´ì½˜
            'footer',  # í‘¸í„° ì´ë¯¸ì§€
            'logo',  # ë¡œê³ 
            'bib-michelin-man',  # ë¯¸ì‰ë¦°ë§¨
            '1star', '2star', '3star',  # ë³„ì  ì•„ì´ì½˜
            'hot', 'close', 'jcb', 'maestro', 'visa', 'amex', 'union'  # ê²°ì œ ì•„ì´ì½˜ë“¤
        ]
        
        # URLì—ì„œ ì œì™¸ íŒ¨í„´ í™•ì¸
        for pattern in exclude_patterns:
            if pattern in url.lower():
                return False
        
        # í´ë˜ìŠ¤ì—ì„œ ì œì™¸ íŒ¨í„´ í™•ì¸
        classes = img_element.get('class', [])
        for cls in classes:
            if any(pattern in cls.lower() for pattern in exclude_patterns):
                return False
        
        # alt í…ìŠ¤íŠ¸ì—ì„œ ì œì™¸ íŒ¨í„´ í™•ì¸
        alt_text = img_element.get('alt', '').lower()
        if any(pattern in alt_text for pattern in exclude_patterns):
            return False
        
        # cloudimg.io ë„ë©”ì¸ì˜ ì´ë¯¸ì§€ëŠ” ìŒì‹ì  ì´ë¯¸ì§€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        if 'cloudimg.io' in url:
            return True
        
        # í¬ê¸°ê°€ ì‘ì€ ì´ë¯¸ì§€ë“¤ ì œì™¸ (ì•„ì´ì½˜ì¼ ê°€ëŠ¥ì„±)
        width = img_element.get('width')
        height = img_element.get('height')
        if width and height:
            try:
                w, h = int(width), int(height)
                if w < 100 or h < 100:  # 100px ë¯¸ë§Œì€ ì•„ì´ì½˜ìœ¼ë¡œ ê°„ì£¼
                    return False
            except ValueError:
                pass
        
        return True
    
    def download_image(self, image_url, restaurant_name, image_index):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_name = re.sub(r'[^\w\-_\.]', '_', restaurant_name)
            safe_name = safe_name[:50]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            
            # ì´ë¯¸ì§€ í™•ì¥ì ì¶”ì¶œ
            parsed_url = urlparse(image_url)
            file_extension = os.path.splitext(parsed_url.path)[1]
            if not file_extension:
                file_extension = '.jpg'  # ê¸°ë³¸ê°’
            
            filename = f"{safe_name}_{image_index:02d}{file_extension}"
            filepath = self.images_dir / filename
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"  âœ“ ì´ë¯¸ì§€ ì €ì¥: {filename}")
            return str(filepath)
            
        except Exception as e:
            print(f"  âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def debug_html_structure(self, soup, restaurant_name):
        """HTML êµ¬ì¡° ë””ë²„ê¹…ì„ ìœ„í•œ í•¨ìˆ˜"""
        print(f"    ğŸ” {restaurant_name} HTML êµ¬ì¡° ë¶„ì„:")
        
        # ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
        all_images = soup.find_all('img')
        print(f"    - ì „ì²´ img íƒœê·¸: {len(all_images)}ê°œ")
        
        # í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ë¶„ì„
        image_classes = {}
        for img in all_images:
            classes = img.get('class', [])
            for cls in classes:
                if cls not in image_classes:
                    image_classes[cls] = 0
                image_classes[cls] += 1
        
        print(f"    - ì´ë¯¸ì§€ í´ë˜ìŠ¤ ë¶„í¬: {image_classes}")
        
        # ì†ì„±ë³„ ë¶„ì„
        attributes = ['ci-src', 'data-src', 'src', 'data-srcset']
        for attr in attributes:
            count = len(soup.find_all('img', {attr: True}))
            if count > 0:
                print(f"    - {attr} ì†ì„±: {count}ê°œ")
        
        # ê°¤ëŸ¬ë¦¬ ê´€ë ¨ ìš”ì†Œ ì°¾ê¸°
        gallery_selectors = ['.gallery', '.image-gallery', '.restaurant-image', '.photo-gallery', '.carousel']
        for selector in gallery_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"    - {selector}: {len(elements)}ê°œ ë°œê²¬")
    
    def scrape_restaurant_images(self, url, restaurant_name):
        """ìŒì‹ì  ì´ë¯¸ì§€ë“¤ ìŠ¤í¬ë˜í•‘ ë° ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ (ì²˜ìŒ ëª‡ ê°œë§Œ)
            if len(self.restaurants) < 3:
                self.debug_html_structure(soup, restaurant_name)
            
            # ì´ë¯¸ì§€ URLë“¤ ì¶”ì¶œ
            image_urls = self.extract_image_urls(soup)
            
            if not image_urls:
                print(f"  âš ï¸ {restaurant_name}: ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            print(f"  ğŸ“¸ {restaurant_name}: {len(image_urls)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
            
            # ì´ë¯¸ì§€ë“¤ ë‹¤ìš´ë¡œë“œ
            downloaded_images = []
            for i, image_url in enumerate(image_urls, 1):
                filepath = self.download_image(image_url, restaurant_name, i)
                if filepath:
                    downloaded_images.append({
                        'url': image_url,
                        'local_path': filepath,
                        'filename': os.path.basename(filepath)
                    })
            
            return downloaded_images
            
        except Exception as e:
            print(f"  âŒ {restaurant_name} ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def scrape_restaurant_detail(self, url):
        """ê°œë³„ ìŒì‹ì  ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìŒì‹ì  ì´ë¦„ - data-sheet__title í´ë˜ìŠ¤ ì‚¬ìš©
            name_element = soup.find('h1', class_='data-sheet__title')
            name = name_element.get_text(strip=True) if name_element else "ì •ë³´ ì—†ìŒ"
            
            # ì£¼ì†Œ - data-sheet__block--text í´ë˜ìŠ¤ì—ì„œ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            address = "ì •ë³´ ì—†ìŒ"
            data_blocks = soup.find_all('div', class_='data-sheet__block--text')
            for block in data_blocks:
                text = block.get_text(strip=True)
                # ì£¼ì†ŒëŠ” ë³´í†µ ì²« ë²ˆì§¸ ë¸”ë¡ì´ê³ , ìˆ«ìì™€ í•œê¸€ì´ í¬í•¨ëœ í˜•íƒœ
                if text and not text.startswith('â‚©') and not text.startswith('Â·') and len(text) > 5:
                    address = text
                    break
            
            # ê°€ê²©ëŒ€ì™€ ì¹´í…Œê³ ë¦¬ - data-sheet__block--textì—ì„œ â‚©ì™€ Â· í¬í•¨ëœ ë¸”ë¡
            price = "ì •ë³´ ì—†ìŒ"
            category = "ì •ë³´ ì—†ìŒ"
            for block in data_blocks:
                text = block.get_text(strip=True)
                if 'â‚©' in text and 'Â·' in text:
                    # â‚© Â· ë„ê°€ë‹ˆíƒ• í˜•íƒœì—ì„œ ë¶„ë¦¬
                    parts = text.split('Â·')
                    if len(parts) >= 2:
                        price_raw = parts[0].strip()  # â‚©
                        category = parts[1].strip()  # ë„ê°€ë‹ˆíƒ•
                        
                        # ê°€ê²©ëŒ€ ì„¤ëª… ì¶”ê°€
                        if price_raw == 'â‚©':
                            price = 'â‚© (ì €ë ´)'
                        elif price_raw == 'â‚©â‚©':
                            price = 'â‚©â‚© (ë³´í†µ)'
                        elif price_raw == 'â‚©â‚©â‚©':
                            price = 'â‚©â‚©â‚© (ë‹¤ì†Œ ê³ ê°€)'
                        elif price_raw == 'â‚©â‚©â‚©â‚©':
                            price = 'â‚©â‚©â‚©â‚© (ê³ ê°€)'
                        else:
                            price = price_raw
                    break
            
            # ë¯¸ìŠë­ ë“±ê¸‰ - data-sheet__classification í´ë˜ìŠ¤ì—ì„œ ì¶”ì¶œ
            rating_parts = []
            
            # classification-itemë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ ë“±ê¸‰ ì •ë³´ ì¶”ì¶œ
            classification_items = soup.find_all('div', class_='data-sheet__classification-item')
            for item in classification_items:
                # í…ìŠ¤íŠ¸ ì„¤ëª…ì—ì„œ ë“±ê¸‰ í™•ì¸ (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
                content_divs = item.find_all('div', class_='data-sheet__classification-item--content')
                for content_div in content_divs:
                    text = content_div.get_text(strip=True)
                    if 'í•œ ê°œì˜ ë³„' in text and '1 Star' not in rating_parts:
                        rating_parts.append('1 Star')
                    elif 'ë‘ ê°œì˜ ë³„' in text and '2 Stars' not in rating_parts:
                        rating_parts.append('2 Stars')
                    elif 'ì„¸ ê°œì˜ ë³„' in text and '3 Stars' not in rating_parts:
                        rating_parts.append('3 Stars')
                    elif 'ë¹• êµ¬ë¥´ë§' in text and 'Bib Gourmand' not in rating_parts:
                        rating_parts.append('Bib Gourmand')
                    elif text == 'New' and 'New' not in rating_parts:
                        rating_parts.append('New')
                    elif 'ìŠ¤ëª° ìˆ' in text and 'Small Shop' not in rating_parts:
                        rating_parts.append('Small Shop')
            
            # í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°ì—ë§Œ ì´ë¯¸ì§€ì—ì„œ í™•ì¸
            if not rating_parts:
                for item in classification_items:
                    icon_span = item.find('span', class_='distinction-icon')
                    if icon_span:
                        # ì´ë¯¸ì§€ íƒœê·¸ì—ì„œ ë³„ì  í™•ì¸
                        img_tag = icon_span.find('img', class_='michelin-award')
                        if img_tag:
                            src = img_tag.get('src', '')
                            if '1star' in src and '1 Star' not in rating_parts:
                                rating_parts.append('1 Star')
                            elif '2star' in src and '2 Stars' not in rating_parts:
                                rating_parts.append('2 Stars')
                            elif '3star' in src and '3 Stars' not in rating_parts:
                                rating_parts.append('3 Stars')
                            elif 'bib-gourmand' in src and 'Bib Gourmand' not in rating_parts:
                                rating_parts.append('Bib Gourmand')
                        else:
                            # i íƒœê·¸ì—ì„œ ìˆ«ì í™•ì¸
                            i_tag = icon_span.find('i', class_='fa-michelin')
                            if i_tag:
                                icon_text = i_tag.get_text(strip=True)
                                if icon_text.isdigit():
                                    if icon_text == '1' and '1 Star' not in rating_parts:
                                        rating_parts.append('1 Star')
                                    elif icon_text == '2' and '2 Stars' not in rating_parts:
                                        rating_parts.append('2 Stars')
                                    elif icon_text == '3' and '3 Stars' not in rating_parts:
                                        rating_parts.append('3 Stars')
            
            rating = ', '.join(rating_parts) if rating_parts else "0 Star, ì¶”ì²œ ë ˆìŠ¤í† ë‘"
            
            # ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘
            print(f"  ğŸ–¼ï¸ {name} ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
            images = self.scrape_restaurant_images(url, name)
            
            restaurant_data = {
                'name': name,
                'address': address,
                'price': price,
                'category': category,
                'rating': rating,
                'url': url,
                'images': images,
                'image_count': len(images)
            }
            
            return restaurant_data
            
        except Exception as e:
            print(f"URL {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def scrape_all_restaurants(self, start_url):
        """ëª¨ë“  ìŒì‹ì  ì •ë³´ ìˆ˜ì§‘"""
        # 1ë‹¨ê³„: ìŒì‹ì  URLë“¤ ìˆ˜ì§‘
        restaurant_urls = self.get_restaurant_urls(start_url)
        
        # 2ë‹¨ê³„: ê° ìŒì‹ì  ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        print("\nìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
        successful_count = 0
        failed_count = 0
        
        for i, url in enumerate(restaurant_urls, 1):
            print(f"\n({i}/{len(restaurant_urls)}) {url} ì²˜ë¦¬ ì¤‘...")
            
            try:
                restaurant_data = self.scrape_restaurant_detail(url)
                if restaurant_data:
                    self.restaurants.append(restaurant_data)
                    successful_count += 1
                    print(f"âœ“ {restaurant_data['name']} ìˆ˜ì§‘ ì™„ë£Œ (ì´ë¯¸ì§€ {restaurant_data.get('image_count', 0)}ê°œ)")
                else:
                    failed_count += 1
                    print(f"âŒ {url} ìˆ˜ì§‘ ì‹¤íŒ¨")
            except Exception as e:
                failed_count += 1
                print(f"âŒ {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 10 == 0:
                print(f"\nğŸ“Š ì§„í–‰ ìƒí™©: {i}/{len(restaurant_urls)} (ì„±ê³µ: {successful_count}, ì‹¤íŒ¨: {failed_count})")
            
            # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(2)  # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œë¡œ ì¸í•´ ê°„ê²© ì¦ê°€
        
        return self.restaurants
    
    def save_to_json(self, filename='michelin_restaurants.json'):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.restaurants, f, ensure_ascii=False, indent=2)
        print(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_to_csv(self, filename='michelin_restaurants.csv'):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.restaurants:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        fieldnames = ['name', 'address', 'price', 'category', 'rating', 'url', 'image_count']
        
        # CSVìš© ë°ì´í„° ì¤€ë¹„ (ì´ë¯¸ì§€ ì •ë³´ëŠ” JSONìœ¼ë¡œ ì €ì¥)
        csv_data = []
        for restaurant in self.restaurants:
            csv_row = {
                'name': restaurant['name'],
                'address': restaurant['address'],
                'price': restaurant['price'],
                'category': restaurant['category'],
                'rating': restaurant['rating'],
                'url': restaurant['url'],
                'image_count': restaurant.get('image_count', 0)
            }
            csv_data.append(csv_row)
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)
        print(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n=== ì´ {len(self.restaurants)}ê°œ ìŒì‹ì  ì •ë³´ ===")
        total_images = sum(restaurant.get('image_count', 0) for restaurant in self.restaurants)
        print(f"ì´ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€: {total_images}ê°œ")
        
        for restaurant in self.restaurants:
            image_info = f"ì´ë¯¸ì§€: {restaurant.get('image_count', 0)}ê°œ"
            if restaurant.get('images'):
                image_info += f" (ì²« ë²ˆì§¸: {restaurant['images'][0]['filename']})"
            
            print(f"""
====
**{restaurant['name']}**
{restaurant['address']}
{restaurant['price']} Â· {restaurant['category']}
ë“±ê¸‰: {restaurant['rating']}
{image_info}
URL: {restaurant['url']}
====
""")

def main():
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = MichelinScraper()
    
    # ì‹œì‘ URL
    start_url = "https://guide.michelin.com/kr/ko/seoul-capital-area/kr-seoul/restaurants?sort=distance"
    
    try:
        # ë°ì´í„° ìˆ˜ì§‘
        restaurants = scraper.scrape_all_restaurants(start_url)
        
        # ê²°ê³¼ ì¶œë ¥
        scraper.print_results()
        
        # íŒŒì¼ë¡œ ì €ì¥
        scraper.save_to_json()
        scraper.save_to_csv()
        
        # ìµœì¢… í†µê³„
        total_images = sum(restaurant.get('image_count', 0) for restaurant in scraper.restaurants)
        print(f"\nğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìŒì‹ì : {len(scraper.restaurants)}ê°œ")
        print(f"ğŸ–¼ï¸ ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"ğŸ“ ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜: {scraper.images_dir.absolute()}")
        
    except KeyboardInterrupt:
        print("\n\nìŠ¤í¬ë˜í•‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"í˜„ì¬ê¹Œì§€ {len(scraper.restaurants)}ê°œ ìŒì‹ì  ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì €ì¥
        if scraper.restaurants:
            scraper.save_to_json('michelin_restaurants_partial.json')
            scraper.save_to_csv('michelin_restaurants_partial.csv')

if __name__ == "__main__":
    main()
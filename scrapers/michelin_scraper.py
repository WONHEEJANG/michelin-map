import requests
from bs4 import BeautifulSoup
import time
import json
import csv
from urllib.parse import urljoin, urlparse
import re
import os
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

class MichelinScraper:
    def __init__(self):
        self.base_url = "https://guide.michelin.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.restaurants = []
        self.images_dir = Path("restaurant_images")
        self.images_dir.mkdir(exist_ok=True)
        self.driver = None
        
    def get_restaurant_urls(self, start_url):
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ëª¨ë“  ìŒì‹ì  URL ìˆ˜ì§‘"""
        print("ìŒì‹ì  URL ìˆ˜ì§‘ ì¤‘...")
        restaurant_urls = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©
        page = 1
        consecutive_empty_pages = 0
        
        while consecutive_empty_pages < 2:  # ì—°ì†ìœ¼ë¡œ 2í˜ì´ì§€ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¤‘ë‹¨
            try:
                # í˜ì´ì§€ë³„ë¡œ URL ìƒì„± (ë¯¸ìŠë­ ê°€ì´ë“œ URL íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì •)
                if page == 1:
                    url = start_url
                else:
                    # ì‹¤ì œ ë¯¸ìŠë­ ê°€ì´ë“œì˜ í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ í™•ì¸
                    url = f"https://guide.michelin.com/kr/ko/seoul-capital-area/kr-seoul/restaurants/page/{page}?sort=distance"
                    
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘: {url}")
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ìŒì‹ì  ì¹´ë“œ ì°¾ê¸° - ì¹´ë“œ ì»¨í…Œì´ë„ˆ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€
                # js-restaurant__list_item í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ì¹´ë“œ ì»¨í…Œì´ë„ˆì—ì„œ ì œëª© ë§í¬ ì¶”ì¶œ
                restaurant_cards = soup.select('.js-restaurant__list_item')
                
                print(f"ì„ íƒì '.js-restaurant__list_item'ë¡œ {len(restaurant_cards)}ê°œ ì¹´ë“œ ë°œê²¬")
                
                restaurant_links = []
                for card in restaurant_cards:
                    # ê° ì¹´ë“œì—ì„œ ì œëª© ë§í¬ë§Œ ì¶”ì¶œ
                    title_link = card.select_one('.card__menu-content--title a[href*="/restaurant/"]')
                    if title_link:
                        restaurant_links.append(title_link)
                
                print(f"ì¹´ë“œì—ì„œ ì¶”ì¶œí•œ ì œëª© ë§í¬: {len(restaurant_links)}ê°œ")
                
                if not restaurant_links:
                    consecutive_empty_pages += 1
                    print(f"í˜ì´ì§€ {page}ì—ì„œ ìŒì‹ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì—°ì† ë¹ˆ í˜ì´ì§€: {consecutive_empty_pages})")
                    page += 1
                    continue
                
                # ì´ í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ë§í¬ë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
                consecutive_empty_pages = 0
                page_urls = []
                
                for link in restaurant_links:
                    href = link.get('href')
                    if href and '/restaurant/' in href:
                        full_url = urljoin(self.base_url, href)
                        
                        # ì „ì²´ ëª©ë¡ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸ (ì œëª© ë§í¬ë§Œ ì¶”ì¶œí•˜ë¯€ë¡œ í˜ì´ì§€ ë‚´ ì¤‘ë³µ ì—†ìŒ)
                        if full_url not in restaurant_urls:
                            restaurant_urls.add(full_url)
                            page_urls.append(full_url)
                
                print(f"í˜ì´ì§€ {page}ì—ì„œ {len(page_urls)}ê°œ ë ˆìŠ¤í† ë‘ ë°œê²¬")
                
                # í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸
                pagination = soup.find('nav', {'aria-label': 'pagination'}) or soup.find('div', class_=re.compile(r'pagination'))
                if pagination:
                    # í˜„ì¬ í˜ì´ì§€ê°€ ë§ˆì§€ë§‰ì¸ì§€ í™•ì¸
                    current_page_elem = pagination.find('span', class_=re.compile(r'current|active')) or \
                                       pagination.find('a', class_=re.compile(r'current|active'))
                    
                    # ë‹¤ìŒ í˜ì´ì§€ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                    all_page_links = pagination.find_all('a')
                    max_page_num = 0
                    for link in all_page_links:
                        try:
                            page_num = int(link.get_text(strip=True))
                            max_page_num = max(max_page_num, page_num)
                        except ValueError:
                            continue
                    
                    if max_page_num > 0 and page >= max_page_num:
                        print(f"í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ {max_page_num}ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                        break
                
                page += 1
                time.sleep(1)  # ìš”ì²­ ê°„ê²©
                
            except Exception as e:
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                consecutive_empty_pages += 1
                page += 1
                if consecutive_empty_pages >= 2:
                    break
        
        restaurant_urls_list = list(restaurant_urls)
        print(f"ì´ {len(restaurant_urls_list)}ê°œ ìŒì‹ì  URL ìˆ˜ì§‘ ì™„ë£Œ")
        return restaurant_urls_list
    
    def setup_selenium_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        if self.driver is None:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            try:
                # webdriver-managerë¥¼ ì‚¬ìš©í•´ì„œ ìë™ìœ¼ë¡œ ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                print("âœ… Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
                return False
        return True
    
    def close_selenium_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            print("âœ… Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
    
    def scrape_images_with_selenium(self, url, restaurant_name):
        """Seleniumì„ ì‚¬ìš©í•´ì„œ ëª¨ë‹¬ì„ ì—´ê³  ëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘"""
        if not self.setup_selenium_driver():
            return []
        
        try:
            print(f"    ğŸŒ Seleniumìœ¼ë¡œ {restaurant_name} í˜ì´ì§€ ë¡œë“œ ì¤‘...")
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
            gallery_selectors = [
                "button.masthead__gallery-open.js-gallery-button",  # ìš°ë˜ì˜¥ì—ì„œ ë°œê²¬ëœ ê°¤ëŸ¬ë¦¬ ë²„íŠ¼
                "button[data-target='#js-gallery-masthead']",  # data-targetìœ¼ë¡œ ì°¾ê¸°
                "button[data-target='#js-modal-gallery']",
                ".js-modal-gallery-trigger",
                "button[aria-label*='gallery']",
                "button[aria-label*='Gallery']",
                ".gallery-trigger",
                ".image-gallery-trigger"
            ]
            
            gallery_button = None
            for selector in gallery_selectors:
                try:
                    gallery_button = WebDriverWait(self.driver, 5).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                    )
                    print(f"    âœ… ê°¤ëŸ¬ë¦¬ ë²„íŠ¼ ë°œê²¬: {selector}")
                    break
                except TimeoutException:
                    continue
            
            if gallery_button:
                # ê°¤ëŸ¬ë¦¬ ë²„íŠ¼ í´ë¦­
                self.driver.execute_script("arguments[0].click();", gallery_button)
                print(f"    ğŸ–¼ï¸ ê°¤ëŸ¬ë¦¬ ëª¨ë‹¬ ì—´ê¸° ì‹œë„...")
                
                # ëª¨ë‹¬ì´ ì—´ë¦´ ë•Œê¹Œì§€ ëŒ€ê¸°
                try:
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".modal__gallery-image"))
                    )
                    print(f"    âœ… ê°¤ëŸ¬ë¦¬ ëª¨ë‹¬ ì—´ë¦¼ í™•ì¸")
                    
                    # ëª¨ë“  ì´ë¯¸ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                    time.sleep(3)
                    
                except TimeoutException:
                    print(f"    âš ï¸ ê°¤ëŸ¬ë¦¬ ëª¨ë‹¬ ì—´ê¸° ì‹¤íŒ¨, ê¸°ë³¸ ì´ë¯¸ì§€ë§Œ ìˆ˜ì§‘")
            else:
                print(f"    âš ï¸ ê°¤ëŸ¬ë¦¬ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ì´ë¯¸ì§€ë§Œ ìˆ˜ì§‘")
            
            # í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_urls = []
            processed_urls = set()
            
            # ci-src ì†ì„±ì´ ìˆëŠ” ëª¨ë“  ì´ë¯¸ì§€ ì°¾ê¸°
            ci_images = self.driver.find_elements(By.CSS_SELECTOR, "img[ci-src]")
            print(f"    ğŸ“¸ ci-src ì†ì„±ì´ ìˆëŠ” ì´ë¯¸ì§€: {len(ci_images)}ê°œ")
            
            for img in ci_images:
                try:
                    url = img.get_attribute('ci-src')
                    if url and url.strip():
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if url.startswith('/'):
                            url = f"https://guide.michelin.com{url}"
                        
                        # í¬ê¸° ì¡°ì • íŒŒë¼ë¯¸í„° ì œê±°
                        if '?' in url:
                            original_url = url.split('?')[0]
                        else:
                            original_url = url
                        
                        if original_url not in processed_urls:
                            # ê°„ë‹¨í•œ í•„í„°ë§ (cloudimg.io ë„ë©”ì¸ë§Œ)
                            if 'cloudimg.io' in original_url:
                                image_urls.append(original_url)
                                processed_urls.add(original_url)
                                print(f"      âœ“ ì´ë¯¸ì§€ ë°œê²¬: {original_url[:60]}...")
                except Exception as e:
                    continue
            
            print(f"    ğŸ“¸ ì´ {len(image_urls)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ URL ì¶”ì¶œ (Selenium)")
            return image_urls
            
        except Exception as e:
            print(f"    âŒ Selenium ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_image_urls(self, soup, restaurant_name):
        """ìŒì‹ì  í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URLë“¤ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        image_urls = []
        processed_urls = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ set
        
        print(f"    ğŸ” {restaurant_name} ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘...")
        
        # ìŒì‹ì ë³„ ê³ ìœ  ì´ë¯¸ì§€ ì„ íƒì (ìš°ì„ ìˆœìœ„ ìˆœ)
        selectors = [
            '.modal__gallery-image img',  # ëª¨ë‹¬ ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ (ê°€ì¥ ì¤‘ìš” - ëª¨ë“  ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€)
            '.owl-item img',  # ìºëŸ¬ì…€ ì•„ì´í…œ ë‚´ ì´ë¯¸ì§€
            '.masthead__gallery img',  # ë©”ì¸ ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€
            '.masthead img',  # ë§ˆìŠ¤íŠ¸í—¤ë“œ ë‚´ ì´ë¯¸ì§€
            '.gallery img',   # ê°¤ëŸ¬ë¦¬ ë‚´ ì´ë¯¸ì§€
            '.image-gallery img',  # ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬
            '.restaurant-image img',  # ë ˆìŠ¤í† ë‘ ì´ë¯¸ì§€
        ]
        
        for selector in selectors:
            image_elements = soup.select(selector)
            print(f"    ì„ íƒì '{selector}': {len(image_elements)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
            
            for img in image_elements:
                # ë‹¤ì–‘í•œ ì†ì„±ì—ì„œ URL ì¶”ì¶œ
                url_attributes = ['ci-src', 'data-src', 'src']
                
                for attr in url_attributes:
                    url = img.get(attr)
                    if url:
                        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if url.startswith('/'):
                            url = f"https://guide.michelin.com{url}"
                        
                        # í¬ê¸° ì¡°ì • íŒŒë¼ë¯¸í„° ì œê±°í•˜ì—¬ ì›ë³¸ URL ì–»ê¸°
                        if '?' in url:
                            original_url = url.split('?')[0]
                        else:
                            original_url = url
                        
                        # ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸
                        if original_url in processed_urls:
                            continue
                        
                        # ìŒì‹ì  ì´ë¯¸ì§€ í•„í„°ë§ (ì•„ì´ì½˜, ë¡œê³  ì œì™¸)
                        if self.is_restaurant_image(original_url, img, restaurant_name):
                            image_urls.append(original_url)
                            processed_urls.add(original_url)
                            print(f"      âœ“ ê³ ìœ  ì´ë¯¸ì§€ ë°œê²¬: {original_url[:60]}...")
                        else:
                            print(f"      âŒ í•„í„°ë§ë¨: {original_url[:60]}...")
        
        # ì¶”ê°€: ëª¨ë“  img íƒœê·¸ì—ì„œ ci-src ì†ì„±ë§Œ ë”°ë¡œ ì°¾ê¸° (JavaScriptë¡œ ë™ì  ë¡œë“œëœ ì´ë¯¸ì§€ë“¤)
        print(f"    ğŸ” ì¶”ê°€ ê²€ìƒ‰: ëª¨ë“  img íƒœê·¸ì—ì„œ ci-src ì†ì„± ì°¾ê¸°...")
        all_ci_images = soup.find_all('img', {'ci-src': True})
        print(f"    ci-src ì†ì„±ì´ ìˆëŠ” ì´ë¯¸ì§€: {len(all_ci_images)}ê°œ")
        
        for img in all_ci_images:
            url = img.get('ci-src')
            if url and url.strip():
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if url.startswith('/'):
                    url = f"https://guide.michelin.com{url}"
                
                # í¬ê¸° ì¡°ì • íŒŒë¼ë¯¸í„° ì œê±°í•˜ì—¬ ì›ë³¸ URL ì–»ê¸°
                if '?' in url:
                    original_url = url.split('?')[0]
                else:
                    original_url = url
                
                # ì´ë¯¸ ì²˜ë¦¬ëœ URLì¸ì§€ í™•ì¸
                if original_url in processed_urls:
                    continue
                
                # ìŒì‹ì  ì´ë¯¸ì§€ í•„í„°ë§
                if self.is_restaurant_image(original_url, img, restaurant_name):
                    image_urls.append(original_url)
                    processed_urls.add(original_url)
                    print(f"      âœ“ ci-src ì´ë¯¸ì§€ ë°œê²¬: {original_url[:60]}...")
                else:
                    print(f"      âŒ ci-src ì´ë¯¸ì§€ í•„í„°ë§ë¨: {original_url[:60]}...")
        
        print(f"    ğŸ“¸ ì´ {len(image_urls)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ URL ì¶”ì¶œ")
        return image_urls
    
    def is_restaurant_image(self, url, img_element, restaurant_name):
        """ìŒì‹ì  ì´ë¯¸ì§€ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)"""
        
        # 1. ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ì¸ ê²½ìš° ìµœìš°ì„ ìœ¼ë¡œ í†µê³¼
        # 'modal__gallery-image' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ë¶€ëª¨ div ì•ˆì— ìˆëŠ” ì´ë¯¸ì§€ëŠ” ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ë¡œ ê°„ì£¼
        parent_div = img_element.find_parent('div', class_='modal__gallery-image')
        if parent_div:
            print(f"      âœ… ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ë¡œ í™•ì¸ë˜ì–´ í†µê³¼: {url[:60]}...")
            return True
        
        # 2. owl-item ë‚´ë¶€ì˜ ì´ë¯¸ì§€ë„ ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ë¡œ ê°„ì£¼
        owl_item = img_element.find_parent('div', class_='owl-item')
        if owl_item:
            print(f"      âœ… ìºëŸ¬ì…€ ì´ë¯¸ì§€ë¡œ í™•ì¸ë˜ì–´ í†µê³¼: {url[:60]}...")
            return True
        
        # 3. ì œì™¸í•  ì´ë¯¸ì§€ íŒ¨í„´ë“¤ (ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì ìš©)
        exclude_patterns = [
            'michelin-award', 'icons/', 'social-', 'footer', 'logo',
            'bib-michelin-man', '1star', '2star', '3star',
            'hot', 'close', 'jcb', 'maestro', 'visa', 'amex', 'union',
            'default', 'placeholder', 'sample'
        ]
        
        # URLì—ì„œ ì œì™¸ íŒ¨í„´ í™•ì¸
        for pattern in exclude_patterns:
            if pattern in url.lower():
                print(f"      âŒ URL íŒ¨í„´ ì œì™¸: {pattern}")
                return False
        
        # í´ë˜ìŠ¤ì—ì„œ ì œì™¸ íŒ¨í„´ í™•ì¸
        classes = img_element.get('class', [])
        for cls in classes:
            if any(pattern in cls.lower() for pattern in exclude_patterns):
                print(f"      âŒ í´ë˜ìŠ¤ íŒ¨í„´ ì œì™¸: {cls}")
                return False
        
        # alt í…ìŠ¤íŠ¸ì—ì„œ ì œì™¸ íŒ¨í„´ í™•ì¸
        alt_text = img_element.get('alt', '').lower()
        if any(pattern in alt_text for pattern in exclude_patterns):
            print(f"      âŒ alt í…ìŠ¤íŠ¸ ì œì™¸: {alt_text}")
            return False
        
        # 4. ê³µí†µ ì´ë¯¸ì§€ URL íŒ¨í„´ë“¤ (ì—¬ëŸ¬ ìŒì‹ì ì—ì„œ ë°˜ë³µ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€)
        common_image_patterns = [
            '0b9dfd084d714be0ad8666feb11efbb3',  # ë¹„ë¹”ëƒ‰ë©´ ì´ë¯¸ì§€
            'a15ac7eea1c6420f9025ce233045161e',  # ë˜ ë‹¤ë¥¸ ê³µí†µ ì´ë¯¸ì§€
            'cab8a8283cd146cda6ca584be6e992c6',  # ë˜ ë‹¤ë¥¸ ê³µí†µ ì´ë¯¸ì§€
        ]
        
        # ê³µí†µ ì´ë¯¸ì§€ íŒ¨í„´ í™•ì¸
        for pattern in common_image_patterns:
            if pattern in url:
                print(f"      âŒ ê³µí†µ ì´ë¯¸ì§€ ì œì™¸: {pattern}")
                return False
        
        # 5. cloudimg.io ë„ë©”ì¸ì˜ ì´ë¯¸ì§€ëŠ” ìŒì‹ì  ì´ë¯¸ì§€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        if 'cloudimg.io' in url:
            print(f"      âœ… cloudimg.io ì´ë¯¸ì§€ë¡œ í†µê³¼: {url[:60]}...")
            return True
        
        # 6. í¬ê¸°ê°€ ì‘ì€ ì´ë¯¸ì§€ë“¤ ì œì™¸ (ì•„ì´ì½˜ì¼ ê°€ëŠ¥ì„±)
        width = img_element.get('width')
        height = img_element.get('height')
        if width and height:
            try:
                w, h = int(width), int(height)
                if w < 100 or h < 100:  # 100px ë¯¸ë§Œì€ ì•„ì´ì½˜ìœ¼ë¡œ ê°„ì£¼
                    print(f"      âŒ í¬ê¸° ë„ˆë¬´ ì‘ìŒ ({w}x{h}): {url[:60]}...")
                    return False
            except ValueError:
                pass
        
        print(f"      âœ… ëª¨ë“  í•„í„° í†µê³¼: {url[:60]}...")
        return True
    
    def download_image(self, image_url, restaurant_name, image_index):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_name = re.sub(r'[^\w\-_\.]', '_', restaurant_name)
            safe_name = safe_name[:50]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            
            # ì´ë¯¸ì§€ í™•ì¥ì ì¶”ì¶œ
            parsed_url = urlparse(image_url)
            file_extension = os.path.splitext(parsed_url.path)[1]
            if not file_extension:
                file_extension = '.jpg'  # ê¸°ë³¸ê°’
            
            filename = f"{safe_name}_{image_index:02d}{file_extension}"
            filepath = self.images_dir / filename
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"  âœ“ ì´ë¯¸ì§€ ì €ì¥: {filename}")
            return str(filepath)
            
        except Exception as e:
            print(f"  âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def debug_html_structure(self, soup, restaurant_name):
        """HTML êµ¬ì¡° ë””ë²„ê¹…ì„ ìœ„í•œ í•¨ìˆ˜"""
        print(f"    ğŸ” {restaurant_name} HTML êµ¬ì¡° ë¶„ì„:")
        
        # ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
        all_images = soup.find_all('img')
        print(f"    - ì „ì²´ img íƒœê·¸: {len(all_images)}ê°œ")
        
        # í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ë¶„ì„
        image_classes = {}
        for img in all_images:
            classes = img.get('class', [])
            for cls in classes:
                if cls not in image_classes:
                    image_classes[cls] = 0
                image_classes[cls] += 1
        
        print(f"    - ì´ë¯¸ì§€ í´ë˜ìŠ¤ ë¶„í¬: {image_classes}")
        
        # ì†ì„±ë³„ ë¶„ì„
        attributes = ['ci-src', 'data-src', 'src', 'data-srcset']
        for attr in attributes:
            count = len(soup.find_all('img', {attr: True}))
            if count > 0:
                print(f"    - {attr} ì†ì„±: {count}ê°œ")
        
        # ê°¤ëŸ¬ë¦¬ ê´€ë ¨ ìš”ì†Œ ì°¾ê¸°
        gallery_selectors = ['.gallery', '.image-gallery', '.restaurant-image', '.photo-gallery', '.carousel']
        for selector in gallery_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"    - {selector}: {len(elements)}ê°œ ë°œê²¬")
    
    def scrape_restaurant_images(self, url, restaurant_name):
        """ìŒì‹ì  ì´ë¯¸ì§€ë“¤ ìŠ¤í¬ë˜í•‘ ë° ë‹¤ìš´ë¡œë“œ (Selenium ì‚¬ìš©)"""
        try:
            print(f"  ğŸ–¼ï¸ {restaurant_name} ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
            
            # ë¨¼ì € Seleniumìœ¼ë¡œ ëª¨ë‹¬ì„ ì—´ê³  ëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œë„
            selenium_image_urls = self.scrape_images_with_selenium(url, restaurant_name)
            
            if selenium_image_urls:
                print(f"  ğŸ“¸ {restaurant_name}: Seleniumìœ¼ë¡œ {len(selenium_image_urls)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                image_urls = selenium_image_urls
            else:
                # Selenium ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                print(f"  ğŸ”„ Selenium ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±...")
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ (ì²˜ìŒ ëª‡ ê°œë§Œ)
                if len(self.restaurants) < 3:
                    self.debug_html_structure(soup, restaurant_name)
                
                # ì´ë¯¸ì§€ URLë“¤ ì¶”ì¶œ
                image_urls = self.extract_image_urls(soup, restaurant_name)
                
                if not image_urls:
                    print(f"  âš ï¸ {restaurant_name}: ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return []
                
                print(f"  ğŸ“¸ {restaurant_name}: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ {len(image_urls)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
            
            # ì´ë¯¸ì§€ë“¤ ë‹¤ìš´ë¡œë“œ
            downloaded_images = []
            for i, image_url in enumerate(image_urls, 1):
                filepath = self.download_image(image_url, restaurant_name, i)
                if filepath:
                    downloaded_images.append({
                        'url': image_url,
                        'local_path': filepath,
                        'filename': os.path.basename(filepath)
                    })
            
            return downloaded_images
            
        except Exception as e:
            print(f"  âŒ {restaurant_name} ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def scrape_restaurant_detail(self, url):
        """ê°œë³„ ìŒì‹ì  ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        try:
            response = self.session.get(url)  
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìŒì‹ì  ì´ë¦„ - data-sheet__title í´ë˜ìŠ¤ ì‚¬ìš©
            name_element = soup.find('h1', class_='data-sheet__title')
            name = name_element.get_text(strip=True) if name_element else "ì •ë³´ ì—†ìŒ"
            
            # ì£¼ì†Œ - data-sheet__block--text í´ë˜ìŠ¤ì—ì„œ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            address = "ì •ë³´ ì—†ìŒ"
            data_blocks = soup.find_all('div', class_='data-sheet__block--text')
            for block in data_blocks:
                text = block.get_text(strip=True)
                # ì£¼ì†ŒëŠ” ë³´í†µ ì²« ë²ˆì§¸ ë¸”ë¡ì´ê³ , ìˆ«ìì™€ í•œê¸€ì´ í¬í•¨ëœ í˜•íƒœ
                if text and not text.startswith('â‚©') and not text.startswith('Â·') and len(text) > 5:
                    address = text
                    break
            
            # ê°€ê²©ëŒ€ì™€ ì¹´í…Œê³ ë¦¬ - data-sheet__block--textì—ì„œ â‚©ì™€ Â· í¬í•¨ëœ ë¸”ë¡
            price = "ì •ë³´ ì—†ìŒ"
            category = "ì •ë³´ ì—†ìŒ"
            for block in data_blocks:
                text = block.get_text(strip=True)
                if 'â‚©' in text and 'Â·' in text:
                    # â‚© Â· ë„ê°€ë‹ˆíƒ• í˜•íƒœì—ì„œ ë¶„ë¦¬
                    parts = text.split('Â·')
                    if len(parts) >= 2:
                        price_raw = parts[0].strip()  # â‚©
                        category = parts[1].strip()  # ë„ê°€ë‹ˆíƒ•
                        
                        # ê°€ê²©ëŒ€ ì„¤ëª… ì¶”ê°€
                        if price_raw == 'â‚©':
                            price = 'â‚© (ì €ë ´)'
                        elif price_raw == 'â‚©â‚©':
                            price = 'â‚©â‚© (ë³´í†µ)'
                        elif price_raw == 'â‚©â‚©â‚©':
                            price = 'â‚©â‚©â‚© (ë‹¤ì†Œ ê³ ê°€)'
                        elif price_raw == 'â‚©â‚©â‚©â‚©':
                            price = 'â‚©â‚©â‚©â‚© (ê³ ê°€)'
                        else:
                            price = price_raw
                    break
            
            # ë¯¸ìŠë­ ë“±ê¸‰ - data-sheet__classification í´ë˜ìŠ¤ì—ì„œ ì¶”ì¶œ
            rating_parts = []
            
            # classification-itemë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ ë“±ê¸‰ ì •ë³´ ì¶”ì¶œ
            classification_items = soup.find_all('div', class_='data-sheet__classification-item')
            for item in classification_items:
                # í…ìŠ¤íŠ¸ ì„¤ëª…ì—ì„œ ë“±ê¸‰ í™•ì¸ (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
                content_divs = item.find_all('div', class_='data-sheet__classification-item--content')
                for content_div in content_divs:
                    text = content_div.get_text(strip=True)
                    if 'í•œ ê°œì˜ ë³„' in text and '1 Star' not in rating_parts:
                        rating_parts.append('1 Star')
                    elif 'ë‘ ê°œì˜ ë³„' in text and '2 Stars' not in rating_parts:
                        rating_parts.append('2 Stars')
                    elif 'ì„¸ ê°œì˜ ë³„' in text and '3 Stars' not in rating_parts:
                        rating_parts.append('3 Stars')
                    elif 'ë¹• êµ¬ë¥´ë§' in text and 'Bib Gourmand' not in rating_parts:
                        rating_parts.append('Bib Gourmand')
                    elif text == 'New' and 'New' not in rating_parts:
                        rating_parts.append('New')
                    elif 'ìŠ¤ëª° ìˆ' in text and 'Small Shop' not in rating_parts:
                        rating_parts.append('Small Shop')
            
            # í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°ì—ë§Œ ì´ë¯¸ì§€ì—ì„œ í™•ì¸
            if not rating_parts:
                for item in classification_items:
                    icon_span = item.find('span', class_='distinction-icon')
                    if icon_span:
                        # ì´ë¯¸ì§€ íƒœê·¸ì—ì„œ ë³„ì  í™•ì¸
                        img_tag = icon_span.find('img', class_='michelin-award')
                        if img_tag:
                            src = img_tag.get('src', '')
                            if '1star' in src and '1 Star' not in rating_parts:
                                rating_parts.append('1 Star')
                            elif '2star' in src and '2 Stars' not in rating_parts:
                                rating_parts.append('2 Stars')
                            elif '3star' in src and '3 Stars' not in rating_parts:
                                rating_parts.append('3 Stars')
                            elif 'bib-gourmand' in src and 'Bib Gourmand' not in rating_parts:
                                rating_parts.append('Bib Gourmand')
                        else:
                            # i íƒœê·¸ì—ì„œ ìˆ«ì í™•ì¸
                            i_tag = icon_span.find('i', class_='fa-michelin')
                            if i_tag:
                                icon_text = i_tag.get_text(strip=True)
                                if icon_text.isdigit():
                                    if icon_text == '1' and '1 Star' not in rating_parts:
                                        rating_parts.append('1 Star')
                                    elif icon_text == '2' and '2 Stars' not in rating_parts:
                                        rating_parts.append('2 Stars')
                                    elif icon_text == '3' and '3 Stars' not in rating_parts:
                                        rating_parts.append('3 Stars')
            
            rating = ', '.join(rating_parts) if rating_parts else "0 Star, ì¶”ì²œ ë ˆìŠ¤í† ë‘"
            
            # ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘
            print(f"  ğŸ–¼ï¸ {name} ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
            images = self.scrape_restaurant_images(url, name)
            
            restaurant_data = {
                'name': name,
                'address': address,
                'price': price,
                'category': category,
                'rating': rating,
                'url': url,
                'images': images,
                'image_count': len(images)
            }
            
            return restaurant_data
            
        except Exception as e:
            print(f"URL {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def scrape_all_restaurants(self, start_url):
        """ëª¨ë“  ìŒì‹ì  ì •ë³´ ìˆ˜ì§‘"""
        # 1ë‹¨ê³„: ìŒì‹ì  URLë“¤ ìˆ˜ì§‘
        restaurant_urls = self.get_restaurant_urls(start_url)
        
        # 2ë‹¨ê³„: ê° ìŒì‹ì  ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        print("\nìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
        successful_count = 0
        failed_count = 0
        
        for i, url in enumerate(restaurant_urls, 1):
            print(f"\n({i}/{len(restaurant_urls)}) {url} ì²˜ë¦¬ ì¤‘...")
            
            try:
                restaurant_data = self.scrape_restaurant_detail(url)
                if restaurant_data:
                    self.restaurants.append(restaurant_data)
                    successful_count += 1
                    print(f"âœ“ {restaurant_data['name']} ìˆ˜ì§‘ ì™„ë£Œ (ì´ë¯¸ì§€ {restaurant_data.get('image_count', 0)}ê°œ)")
                else:
                    failed_count += 1
                    print(f"âŒ {url} ìˆ˜ì§‘ ì‹¤íŒ¨")
            except Exception as e:
                failed_count += 1
                print(f"âŒ {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 10 == 0:
                print(f"\nğŸ“Š ì§„í–‰ ìƒí™©: {i}/{len(restaurant_urls)} (ì„±ê³µ: {successful_count}, ì‹¤íŒ¨: {failed_count})")
            
            # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(2)  # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œë¡œ ì¸í•´ ê°„ê²© ì¦ê°€
        
        return self.restaurants
    
    def save_to_json(self, filename='michelin_restaurants.json'):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.restaurants, f, ensure_ascii=False, indent=2)
        print(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_to_csv(self, filename='michelin_restaurants.csv'):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.restaurants:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        fieldnames = ['name', 'address', 'price', 'category', 'rating', 'url', 'image_count']
        
        # CSVìš© ë°ì´í„° ì¤€ë¹„ (ì´ë¯¸ì§€ ì •ë³´ëŠ” JSONìœ¼ë¡œ ì €ì¥)
        csv_data = []
        for restaurant in self.restaurants:
            csv_row = {
                'name': restaurant['name'],
                'address': restaurant['address'],
                'price': restaurant['price'],
                'category': restaurant['category'],
                'rating': restaurant['rating'],
                'url': restaurant['url'],
                'image_count': restaurant.get('image_count', 0)
            }
            csv_data.append(csv_row)
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)
        print(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n=== ì´ {len(self.restaurants)}ê°œ ìŒì‹ì  ì •ë³´ ===")
        total_images = sum(restaurant.get('image_count', 0) for restaurant in self.restaurants)
        print(f"ì´ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€: {total_images}ê°œ")
        
        for restaurant in self.restaurants:
            image_info = f"ì´ë¯¸ì§€: {restaurant.get('image_count', 0)}ê°œ"
            if restaurant.get('images'):
                image_info += f" (ì²« ë²ˆì§¸: {restaurant['images'][0]['filename']})"
            
            print(f"""
====
**{restaurant['name']}**
{restaurant['address']}
{restaurant['price']} Â· {restaurant['category']}
ë“±ê¸‰: {restaurant['rating']}
{image_info}
URL: {restaurant['url']}
====
""")

def main():
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = MichelinScraper()
    
    # ì‹œì‘ URL
    start_url = "https://guide.michelin.com/kr/ko/seoul-capital-area/kr-seoul/restaurants?sort=distance"
    
    try:
        # ë°ì´í„° ìˆ˜ì§‘
        restaurants = scraper.scrape_all_restaurants(start_url)
        
        # ê²°ê³¼ ì¶œë ¥
        scraper.print_results()
        
        # íŒŒì¼ë¡œ ì €ì¥
        scraper.save_to_json()
        scraper.save_to_csv()
        
        # ìµœì¢… í†µê³„
        total_images = sum(restaurant.get('image_count', 0) for restaurant in scraper.restaurants)
        print(f"\nğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìŒì‹ì : {len(scraper.restaurants)}ê°œ")
        print(f"ğŸ–¼ï¸ ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"ğŸ“ ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜: {scraper.images_dir.absolute()}")
        
    except KeyboardInterrupt:
        print("\n\nìŠ¤í¬ë˜í•‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"í˜„ì¬ê¹Œì§€ {len(scraper.restaurants)}ê°œ ìŒì‹ì  ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì €ì¥
        if scraper.restaurants:
            scraper.save_to_json('michelin_restaurants_partial.json')
            scraper.save_to_csv('michelin_restaurants_partial.csv')

if __name__ == "__main__":
    main()
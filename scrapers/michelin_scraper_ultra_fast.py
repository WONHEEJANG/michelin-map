import requests
from bs4 import BeautifulSoup
import time
import json
import csv
from urllib.parse import urljoin, urlparse
import re
import os
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from concurrent.futures import ThreadPoolExecutor
import threading
from queue import Queue

class UltraFastMichelinScraper:
    def __init__(self, max_workers=4, driver_pool_size=4):
        self.base_url = "https://guide.michelin.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.restaurants = []
        self.images_dir = Path("restaurant_images")
        self.images_dir.mkdir(exist_ok=True)
        
        # ì›Œì»¤ ìˆ˜ ì„¤ì •
        self.max_workers = max_workers
        self.driver_pool_size = driver_pool_size
        
        # Selenium ë“œë¼ì´ë²„ í’€
        self.driver_pool = Queue(maxsize=driver_pool_size)
        self.driver_lock = threading.Lock()
        
        # ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™”
        self._initialize_driver_pool()
        
        print(f"ğŸš€ ìš¸íŠ¸ë¼ ë¹ ë¥¸ ìŠ¤í¬ë˜í¼ ì„¤ì •: {max_workers}ê°œ ì›Œì»¤, {driver_pool_size}ê°œ ë“œë¼ì´ë²„ í’€")
    
    def _initialize_driver_pool(self):
        """Selenium ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™”"""
        print("ğŸ”§ Selenium ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™” ì¤‘...")
        
        for i in range(self.driver_pool_size):
            driver = self._create_driver()
            if driver:
                self.driver_pool.put(driver)
                print(f"  âœ… ë“œë¼ì´ë²„ {i+1}/{self.driver_pool_size} ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print(f"  âŒ ë“œë¼ì´ë²„ {i+1}/{self.driver_pool_size} ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        print(f"ğŸ¯ ì´ {self.driver_pool.qsize()}ê°œ ë“œë¼ì´ë²„ í’€ ì¤€ë¹„ ì™„ë£Œ")
    
    def _create_driver(self):
        """ìƒˆë¡œìš´ Selenium ë“œë¼ì´ë²„ ìƒì„±"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            return driver
        except Exception as e:
            print(f"âŒ Selenium ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _get_driver_from_pool(self):
        """ë“œë¼ì´ë²„ í’€ì—ì„œ ë“œë¼ì´ë²„ ê°€ì ¸ì˜¤ê¸°"""
        try:
            driver = self.driver_pool.get(timeout=10)  # 10ì´ˆ ëŒ€ê¸°
            return driver
        except:
            # í’€ì´ ë¹„ì–´ìˆìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            return self._create_driver()
    
    def _return_driver_to_pool(self, driver):
        """ë“œë¼ì´ë²„ë¥¼ í’€ì— ë°˜í™˜"""
        if driver:
            try:
                self.driver_pool.put_nowait(driver)
            except:
                # í’€ì´ ê°€ë“ ì°¨ë©´ ë“œë¼ì´ë²„ ì¢…ë£Œ
                try:
                    driver.quit()
                except:
                    pass
    
    def get_restaurant_urls(self, start_url):
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ëª¨ë“  ìŒì‹ì  URL ìˆ˜ì§‘"""
        print("ìŒì‹ì  URL ìˆ˜ì§‘ ì¤‘...")
        restaurant_urls = set()
        page = 1
        consecutive_empty_pages = 0
        
        while consecutive_empty_pages < 2:
            try:
                if page == 1:
                    url = start_url
                else:
                    url = f"https://guide.michelin.com/kr/ko/seoul-capital-area/kr-seoul/restaurants/page/{page}?sort=distance"
                    
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘: {url}")
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ìŒì‹ì  ì¹´ë“œ ì°¾ê¸°
                restaurant_cards = soup.select('.js-restaurant__list_item')
                print(f"ì„ íƒì '.js-restaurant__list_item'ë¡œ {len(restaurant_cards)}ê°œ ì¹´ë“œ ë°œê²¬")
                
                restaurant_links = []
                for card in restaurant_cards:
                    title_link = card.select_one('.card__menu-content--title a[href*="/restaurant/"]')
                    if title_link:
                        restaurant_links.append(title_link)
                
                print(f"ì¹´ë“œì—ì„œ ì¶”ì¶œí•œ ì œëª© ë§í¬: {len(restaurant_links)}ê°œ")
                
                if not restaurant_links:
                    consecutive_empty_pages += 1
                    print(f"í˜ì´ì§€ {page}ì—ì„œ ìŒì‹ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì—°ì† ë¹ˆ í˜ì´ì§€: {consecutive_empty_pages})")
                    page += 1
                    continue
                
                consecutive_empty_pages = 0
                page_urls = []
                
                for link in restaurant_links:
                    href = link.get('href')
                    if href and '/restaurant/' in href:
                        full_url = urljoin(self.base_url, href)
                        if full_url not in restaurant_urls:
                            restaurant_urls.add(full_url)
                            page_urls.append(full_url)
                
                print(f"í˜ì´ì§€ {page}ì—ì„œ {len(page_urls)}ê°œ ë ˆìŠ¤í† ë‘ ë°œê²¬")
                
                # í˜ì´ì§€ë„¤ì´ì…˜ í™•ì¸
                pagination = soup.find('nav', {'aria-label': 'pagination'}) or soup.find('div', class_=re.compile(r'pagination'))
                if pagination:
                    all_page_links = pagination.find_all('a')
                    max_page_num = 0
                    for link in all_page_links:
                        try:
                            page_num = int(link.get_text(strip=True))
                            max_page_num = max(max_page_num, page_num)
                        except ValueError:
                            continue
                    
                    if max_page_num > 0 and page >= max_page_num:
                        print(f"í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ {max_page_num}ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                        break
                
                page += 1
                time.sleep(0.3)  # ìš”ì²­ ê°„ê²© ë” ë‹¨ì¶•
                
            except Exception as e:
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                consecutive_empty_pages += 1
                page += 1
                if consecutive_empty_pages >= 2:
                    break
        
        restaurant_urls_list = list(restaurant_urls)
        print(f"ì´ {len(restaurant_urls_list)}ê°œ ìŒì‹ì  URL ìˆ˜ì§‘ ì™„ë£Œ")
        return restaurant_urls_list
    
    def scrape_images_with_selenium_pool(self, url, restaurant_name):
        """ë“œë¼ì´ë²„ í’€ì„ ì‚¬ìš©í•´ì„œ ì´ë¯¸ì§€ ìˆ˜ì§‘"""
        driver = self._get_driver_from_pool()
        if not driver:
            return []
        
        try:
            print(f"    ğŸŒ Seleniumìœ¼ë¡œ {restaurant_name} í˜ì´ì§€ ë¡œë“œ ì¤‘...")
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ê°¤ëŸ¬ë¦¬ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
            gallery_selectors = [
                "button.masthead__gallery-open.js-gallery-button",
                "button[data-target='#js-gallery-masthead']",
                "button[data-target='#js-modal-gallery']",
                ".js-modal-gallery-trigger",
                "button[aria-label*='gallery']",
                "button[aria-label*='Gallery']",
                ".gallery-trigger",
                ".image-gallery-trigger"
            ]
            
            gallery_button = None
            for selector in gallery_selectors:
                try:
                    gallery_button = WebDriverWait(driver, 5).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                    )
                    print(f"    âœ… ê°¤ëŸ¬ë¦¬ ë²„íŠ¼ ë°œê²¬: {selector}")
                    break
                except TimeoutException:
                    continue
            
            if gallery_button:
                driver.execute_script("arguments[0].click();", gallery_button)
                print(f"    ğŸ–¼ï¸ ê°¤ëŸ¬ë¦¬ ëª¨ë‹¬ ì—´ê¸° ì‹œë„...")
                
                try:
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".modal__gallery-image"))
                    )
                    print(f"    âœ… ê°¤ëŸ¬ë¦¬ ëª¨ë‹¬ ì—´ë¦¼ í™•ì¸")
                    time.sleep(1.5)  # ì´ë¯¸ì§€ ë¡œë“œ ëŒ€ê¸° ë‹¨ì¶•
                except TimeoutException:
                    print(f"    âš ï¸ ê°¤ëŸ¬ë¦¬ ëª¨ë‹¬ ì—´ê¸° ì‹¤íŒ¨, ê¸°ë³¸ ì´ë¯¸ì§€ë§Œ ìˆ˜ì§‘")
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_urls = []
            processed_urls = set()
            
            ci_images = driver.find_elements(By.CSS_SELECTOR, "img[ci-src]")
            print(f"    ğŸ“¸ ci-src ì†ì„±ì´ ìˆëŠ” ì´ë¯¸ì§€: {len(ci_images)}ê°œ")
            
            for img in ci_images:
                try:
                    url = img.get_attribute('ci-src')
                    if url and url.strip():
                        if url.startswith('/'):
                            url = f"https://guide.michelin.com{url}"
                        
                        if '?' in url:
                            original_url = url.split('?')[0]
                        else:
                            original_url = url
                        
                        if original_url not in processed_urls:
                            if 'cloudimg.io' in original_url:
                                image_urls.append(original_url)
                                processed_urls.add(original_url)
                                print(f"      âœ“ ì´ë¯¸ì§€ ë°œê²¬: {original_url[:60]}...")
                except Exception as e:
                    continue
            
            print(f"    ğŸ“¸ ì´ {len(image_urls)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ URL ì¶”ì¶œ (Selenium)")
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            downloaded_images = []
            for i, image_url in enumerate(image_urls, 1):
                filepath = self.download_image(image_url, restaurant_name, i)
                if filepath:
                    downloaded_images.append({
                        'url': image_url,
                        'local_path': filepath,
                        'filename': os.path.basename(filepath)
                    })
            
            return downloaded_images
            
        except Exception as e:
            print(f"    âŒ Selenium ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
        finally:
            # ë“œë¼ì´ë²„ë¥¼ í’€ì— ë°˜í™˜
            self._return_driver_to_pool(driver)
    
    def download_image(self, image_url, restaurant_name, image_index):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
        try:
            safe_name = re.sub(r'[^\w\-_\.]', '_', restaurant_name)
            safe_name = safe_name[:50]
            
            parsed_url = urlparse(image_url)
            file_extension = os.path.splitext(parsed_url.path)[1]
            if not file_extension:
                file_extension = '.jpg'
            
            filename = f"{safe_name}_{image_index:02d}{file_extension}"
            filepath = self.images_dir / filename
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = self.session.get(image_url, timeout=20)  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"  âœ“ ì´ë¯¸ì§€ ì €ì¥: {filename}")
            return str(filepath)
            
        except Exception as e:
            print(f"  âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def scrape_restaurant_detail(self, url):
        """ê°œë³„ ìŒì‹ì  ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘"""
        try:
            response = self.session.get(url)  
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìŒì‹ì  ì´ë¦„
            name_element = soup.find('h1', class_='data-sheet__title')
            name = name_element.get_text(strip=True) if name_element else "ì •ë³´ ì—†ìŒ"
            
            # ì£¼ì†Œ
            address = "ì •ë³´ ì—†ìŒ"
            data_blocks = soup.find_all('div', class_='data-sheet__block--text')
            for block in data_blocks:
                text = block.get_text(strip=True)
                if text and not text.startswith('â‚©') and not text.startswith('Â·') and len(text) > 5:
                    address = text
                    break
            
            # ê°€ê²©ëŒ€ì™€ ì¹´í…Œê³ ë¦¬
            price = "ì •ë³´ ì—†ìŒ"
            category = "ì •ë³´ ì—†ìŒ"
            for block in data_blocks:
                text = block.get_text(strip=True)
                if 'â‚©' in text and 'Â·' in text:
                    parts = text.split('Â·')
                    if len(parts) >= 2:
                        price_raw = parts[0].strip()
                        category = parts[1].strip()
                        
                        if price_raw == 'â‚©':
                            price = 'â‚© (ì €ë ´)'
                        elif price_raw == 'â‚©â‚©':
                            price = 'â‚©â‚© (ë³´í†µ)'
                        elif price_raw == 'â‚©â‚©â‚©':
                            price = 'â‚©â‚©â‚© (ë‹¤ì†Œ ê³ ê°€)'
                        elif price_raw == 'â‚©â‚©â‚©â‚©':
                            price = 'â‚©â‚©â‚©â‚© (ê³ ê°€)'
                        else:
                            price = price_raw
                    break
            
            # ë¯¸ìŠë­ ë“±ê¸‰
            rating_parts = []
            classification_items = soup.find_all('div', class_='data-sheet__classification-item')
            for item in classification_items:
                content_divs = item.find_all('div', class_='data-sheet__classification-item--content')
                for content_div in content_divs:
                    text = content_div.get_text(strip=True)
                    if 'í•œ ê°œì˜ ë³„' in text and '1 Star' not in rating_parts:
                        rating_parts.append('1 Star')
                    elif 'ë‘ ê°œì˜ ë³„' in text and '2 Stars' not in rating_parts:
                        rating_parts.append('2 Stars')
                    elif 'ì„¸ ê°œì˜ ë³„' in text and '3 Stars' not in rating_parts:
                        rating_parts.append('3 Stars')
                    elif 'ë¹• êµ¬ë¥´ë§' in text and 'Bib Gourmand' not in rating_parts:
                        rating_parts.append('Bib Gourmand')
                    elif text == 'New' and 'New' not in rating_parts:
                        rating_parts.append('New')
                    elif 'ìŠ¤ëª° ìˆ' in text and 'Small Shop' not in rating_parts:
                        rating_parts.append('Small Shop')
            
            rating = ', '.join(rating_parts) if rating_parts else "0 Star, ì¶”ì²œ ë ˆìŠ¤í† ë‘"
            
            # ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘ (ë“œë¼ì´ë²„ í’€ ì‚¬ìš©)
            print(f"  ğŸ–¼ï¸ {name} ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
            images = self.scrape_images_with_selenium_pool(url, name)
            
            restaurant_data = {
                'name': name,
                'address': address,
                'price': price,
                'category': category,
                'rating': rating,
                'url': url,
                'images': images,
                'image_count': len(images)
            }
            
            return restaurant_data
            
        except Exception as e:
            print(f"URL {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def save_to_json(self, filename='michelin_restaurants_ultra.json'):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.restaurants, f, ensure_ascii=False, indent=2)
        print(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_to_csv(self, filename='michelin_restaurants_ultra.csv'):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.restaurants:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        fieldnames = ['name', 'address', 'price', 'category', 'rating', 'url', 'image_count']
        
        csv_data = []
        for restaurant in self.restaurants:
            csv_row = {
                'name': restaurant['name'],
                'address': restaurant['address'],
                'price': restaurant['price'],
                'category': restaurant['category'],
                'rating': restaurant['rating'],
                'url': restaurant['url'],
                'image_count': restaurant.get('image_count', 0)
            }
            csv_data.append(csv_row)
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)
        print(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def scrape_single_restaurant_ultra(args):
    """ë‹¨ì¼ ìŒì‹ì  ìŠ¤í¬ë˜í•‘ (ìš¸íŠ¸ë¼ ë¹ ë¥¸ ë²„ì „)"""
    url, scraper_instance = args
    
    try:
        print(f"ğŸ”„ {url} ì²˜ë¦¬ ì¤‘...")
        restaurant_data = scraper_instance.scrape_restaurant_detail(url)
        
        if restaurant_data:
            print(f"âœ“ {restaurant_data['name']} ìˆ˜ì§‘ ì™„ë£Œ (ì´ë¯¸ì§€ {restaurant_data.get('image_count', 0)}ê°œ)")
            return restaurant_data
        else:
            print(f"âŒ {url} ìˆ˜ì§‘ ì‹¤íŒ¨")
            return None
            
    except Exception as e:
        print(f"âŒ {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def main():
    # ìš¸íŠ¸ë¼ ë¹ ë¥¸ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = UltraFastMichelinScraper(max_workers=4, driver_pool_size=4)
    
    # ì‹œì‘ URL
    start_url = "https://guide.michelin.com/kr/ko/seoul-capital-area/kr-seoul/restaurants?sort=distance"
    
    try:
        # 1ë‹¨ê³„: ìŒì‹ì  URLë“¤ ìˆ˜ì§‘
        restaurant_urls = scraper.get_restaurant_urls(start_url)
        
        # 2ë‹¨ê³„: ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        print(f"\nìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘... (ìš¸íŠ¸ë¼ ë¹ ë¥¸ ë©€í‹°ìŠ¤ë ˆë”©: {scraper.max_workers}ê°œ ì›Œì»¤)")
        
        start_time = time.time()
        
        # ë°°ì¹˜ë¡œ ì²˜ë¦¬
        batch_size = scraper.max_workers * 2
        successful_count = 0
        failed_count = 0
        
        for i in range(0, len(restaurant_urls), batch_size):
            batch_urls = restaurant_urls[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(restaurant_urls) + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_urls)}ê°œ)")
            
            # ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=scraper.max_workers) as executor:
                args = [(url, scraper) for url in batch_urls]
                results = list(executor.map(scrape_single_restaurant_ultra, args))
            
            # ê²°ê³¼ ì²˜ë¦¬
            for result in results:
                if result:
                    scraper.restaurants.append(result)
                    successful_count += 1
                else:
                    failed_count += 1
            
            print(f"ğŸ“Š ë°°ì¹˜ {batch_num} ì™„ë£Œ: ì„±ê³µ {len([r for r in results if r])}ê°œ")
            
            # ë°°ì¹˜ ê°„ ëŒ€ê¸°
            if i + batch_size < len(restaurant_urls):
                time.sleep(0.5)  # ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print(f"\nğŸ‰ ìš¸íŠ¸ë¼ ë¹ ë¥¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print(f"ğŸ“Š ì´ ìŒì‹ì : {len(scraper.restaurants)}ê°œ")
        print(f"âœ… ì„±ê³µ: {successful_count}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        print(f"âš¡ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {elapsed_time/len(restaurant_urls):.2f}ì´ˆ/ê°œ")
        
        # íŒŒì¼ë¡œ ì €ì¥
        scraper.save_to_json()
        scraper.save_to_csv()
        
        # ìµœì¢… í†µê³„
        total_images = sum(restaurant.get('image_count', 0) for restaurant in scraper.restaurants)
        print(f"\nğŸ‰ ìš¸íŠ¸ë¼ ë¹ ë¥¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìŒì‹ì : {len(scraper.restaurants)}ê°œ")
        print(f"ğŸ–¼ï¸ ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"ğŸ“ ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜: {scraper.images_dir.absolute()}")
        
    except KeyboardInterrupt:
        print("\n\nìŠ¤í¬ë˜í•‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"í˜„ì¬ê¹Œì§€ {len(scraper.restaurants)}ê°œ ìŒì‹ì  ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì €ì¥
        if scraper.restaurants:
            scraper.save_to_json('michelin_restaurants_ultra_partial.json')
            scraper.save_to_csv('michelin_restaurants_ultra_partial.csv')
    
    finally:
        # ë“œë¼ì´ë²„ í’€ ì •ë¦¬
        print("ğŸ§¹ ë“œë¼ì´ë²„ í’€ ì •ë¦¬ ì¤‘...")
        while not scraper.driver_pool.empty():
            try:
                driver = scraper.driver_pool.get_nowait()
                driver.quit()
            except:
                pass
        print("âœ… ë“œë¼ì´ë²„ í’€ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()
